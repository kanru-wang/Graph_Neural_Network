{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanru-wang/Graph_Neural_Network/blob/main/Node_Classification_Subgraph_Partition_Customized_Aggregation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PubMed Node Classification with Subgraph Partitioning and Customized Message-Passing Aggregations\n",
        "\n",
        "Given the ground-truth labels of a small subset of nodes, the task is to infer the labels for all the remaining nodes.\n",
        "\n",
        "See: https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html"
      ],
      "metadata": {
        "id": "_DiyjLEp3VAT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWn5yzT0LOzH",
        "outputId": "03d8a96d-6625-47a3-fe6b-bfa99782a1f0"
      },
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `PubMed` dataset has 3 Classes. The number of node features is 500."
      ],
      "metadata": {
        "id": "PDww5LoFBBBQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBN2pGDueDpZ",
        "outputId": "b0b06c73-434e-455c-d749-217a1b7bb4aa"
      },
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "dataset = Planetoid(root='data/Planetoid', name='PubMed', transform=NormalizeFeatures())\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('==================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('===============================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: PubMed():\n",
            "==================\n",
            "Number of graphs: 1\n",
            "Number of features: 500\n",
            "Number of classes: 3\n",
            "\n",
            "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n",
            "===============================================================================================================\n",
            "Number of nodes: 19717\n",
            "Number of edges: 88648\n",
            "Average node degree: 4.50\n",
            "Number of training nodes: 60\n",
            "Training node label rate: 0.003\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp__L20M-qns"
      },
      "source": [
        "## Cluster-GCN Subgraph Partitioning\n",
        "\n",
        "Without Cluster-GCN, we would have to train GNNs for node classification tasks only in a full-batch fashion. Every node's hidden representation is computed in parallel and is available to re-use in the next layer. But the problem is that this method takes too much GPU memory.\n",
        "\n",
        "Cluster-GCN pre-partitions the graph into subgraphs on which one can operate in a mini-batch fashion. GNNs are restricted to solely convolve inside their specific subgraphs.\n",
        "\n",
        "However, after the graph is partitioned, some links are removed which may limit the model's performance due to a biased estimation. To address this issue, Cluster-GCN also incorporates between-cluster links inside a mini-batch, which results in the stochastic partitioning scheme in below. Here, colors represent the adjacency information that is maintained per batch (which is potentially different for every epoch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QfBQ_jj_24E"
      },
      "source": [
        "![Screen Shot 2020-08-27 at 14.58.15.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwuwXWLZ_4OQ"
      },
      "source": [
        "The Cluster-GCN algorithm has two stages:\n",
        "1. `ClusterData` converts a `Data` object into a dataset of subgraphs containing `num_parts` partitions.\n",
        "2. Given a user-defined `batch_size`, `ClusterLoader` implements the stochastic partitioning scheme (chart above) in order to create mini-batches.\n",
        "\n",
        "Mini-batches are created in below. Here, we partition the initial graph into 128 partitions, and use a `batch_size` of 32 subgraphs to form mini-batches (leaving us with 4 batches per epoch). After a single epoch, each node has been seen exactly once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwhcs1L5fPjx",
        "outputId": "01c617da-cc32-4e85-c494-e36dbb67c28e"
      },
      "source": [
        "from torch_geometric.loader import ClusterData, ClusterLoader\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "cluster_data = ClusterData(data, num_parts=128)  # 1. Create subgraphs.\n",
        "train_loader = ClusterLoader(cluster_data, batch_size=32, shuffle=True)  # 2. Stochastic partioning scheme.\n",
        "\n",
        "print()\n",
        "total_num_nodes = 0\n",
        "for step, sub_data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of nodes in the current batch: {sub_data.num_nodes}')\n",
        "    print(sub_data)\n",
        "    print()\n",
        "    total_num_nodes += sub_data.num_nodes\n",
        "\n",
        "print(f'Iterated over {total_num_nodes} of {data.num_nodes} nodes!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing METIS partitioning...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1:\n",
            "=======\n",
            "Number of nodes in the current batch: 4928\n",
            "Data(x=[4928, 500], y=[4928], train_mask=[4928], val_mask=[4928], test_mask=[4928], edge_index=[2, 16174])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of nodes in the current batch: 4937\n",
            "Data(x=[4937, 500], y=[4937], train_mask=[4937], val_mask=[4937], test_mask=[4937], edge_index=[2, 17832])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of nodes in the current batch: 4927\n",
            "Data(x=[4927, 500], y=[4927], train_mask=[4927], val_mask=[4927], test_mask=[4927], edge_index=[2, 14712])\n",
            "\n",
            "Step 4:\n",
            "=======\n",
            "Number of nodes in the current batch: 4925\n",
            "Data(x=[4925, 500], y=[4925], train_mask=[4925], val_mask=[4925], test_mask=[4925], edge_index=[2, 18006])\n",
            "\n",
            "Iterated over 19717 of 19717 nodes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With or without Cluster-GCN, the code to implement GNN is the same:"
      ],
      "metadata": {
        "id": "wZ8Pfo239CJt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1ho60LwfkHR",
        "outputId": "2d53c011-aa99-4d79-d0ae-dff1921fa62f"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(500, 16)\n",
            "  (conv2): GCNConv(16, 3)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0JHjtY-CCkW"
      },
      "source": [
        "Iterate over each mini-batch, and optimize each batch independently from each other:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "wLxawwYlgjDb",
        "outputId": "28666719-1140-4fdc-c99d-e90014f5405f"
      },
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "def train(model):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for sub_data in train_loader:\n",
        "        out = model(sub_data.x, sub_data.edge_index)\n",
        "        loss = criterion(out[sub_data.train_mask], sub_data.y[sub_data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    pred = out.argmax(dim=1)\n",
        "    accs = []\n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "        correct = pred[mask] == data.y[mask]\n",
        "        accs.append(int(correct.sum()) / int(mask.sum()))\n",
        "    return accs\n",
        "\n",
        "model = GCN(hidden_channels=16)\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    loss = train(model)\n",
        "    train_acc, val_acc, test_acc = test(model)\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.3333, Val Acc: 0.4160, Test Acc: 0.4070\n",
            "Epoch: 002, Train: 0.7167, Val Acc: 0.5600, Test Acc: 0.5500\n",
            "Epoch: 003, Train: 0.6500, Val Acc: 0.5180, Test Acc: 0.5000\n",
            "Epoch: 004, Train: 0.8833, Val Acc: 0.6340, Test Acc: 0.6290\n",
            "Epoch: 005, Train: 0.9500, Val Acc: 0.7660, Test Acc: 0.7500\n",
            "Epoch: 006, Train: 0.9000, Val Acc: 0.6940, Test Acc: 0.6680\n",
            "Epoch: 007, Train: 0.7833, Val Acc: 0.6560, Test Acc: 0.6830\n",
            "Epoch: 008, Train: 0.9333, Val Acc: 0.7560, Test Acc: 0.7340\n",
            "Epoch: 009, Train: 0.9500, Val Acc: 0.7660, Test Acc: 0.7450\n",
            "Epoch: 010, Train: 0.9500, Val Acc: 0.7200, Test Acc: 0.6980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "## Customizing Aggregations within Message Passing\n",
        "\n",
        "The performance of GNNs with different aggregation functions differs when applied to distinct tasks and datasets. Using multiple aggregations can potentially gain substantial improvements. It converges much faster than just using mean aggregation."
      ],
      "metadata": {
        "id": "gBT58f6nvCcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import (\n",
        "    Aggregation,\n",
        "    MaxAggregation,\n",
        "    MeanAggregation,\n",
        "    MultiAggregation,\n",
        "    SAGEConv,\n",
        "    SoftmaxAggregation,\n",
        "    StdAggregation,\n",
        "    SumAggregation,\n",
        "    VarAggregation,\n",
        ")\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, aggr='mean', aggr_kwargs=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(\n",
        "            dataset.num_node_features,\n",
        "            hidden_channels,\n",
        "            aggr=aggr,\n",
        "            aggr_kwargs=aggr_kwargs,\n",
        "        )\n",
        "        self.conv2 = SAGEConv(\n",
        "            hidden_channels,\n",
        "            dataset.num_classes,\n",
        "            aggr=copy.deepcopy(aggr),\n",
        "            aggr_kwargs=aggr_kwargs,\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Fvw4n6D-LhN2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(12345)\n",
        "\n",
        "model = GNN(\n",
        "    hidden_channels=16,\n",
        "    aggr=[\n",
        "        MeanAggregation(),\n",
        "        MaxAggregation(),\n",
        "        SumAggregation(),\n",
        "        StdAggregation(),\n",
        "        VarAggregation(),\n",
        "    ]\n",
        ")\n",
        "print(model)\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    loss = train(model)\n",
        "    train_acc, val_acc, test_acc = test(model)\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzEQ6wwSkINF",
        "outputId": "ca3d25db-7d58-4af2-8459-21fffc59d3cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (conv1): SAGEConv(500, 16, aggr=['MeanAggregation()', 'MaxAggregation()', 'SumAggregation()', 'StdAggregation()', 'VarAggregation()'])\n",
            "  (conv2): SAGEConv(16, 3, aggr=['MeanAggregation()', 'MaxAggregation()', 'SumAggregation()', 'StdAggregation()', 'VarAggregation()'])\n",
            ")\n",
            "Epoch: 001, Train: 0.6333, Val Acc: 0.4100, Test Acc: 0.3960\n",
            "Epoch: 002, Train: 0.8333, Val Acc: 0.6700, Test Acc: 0.6440\n",
            "Epoch: 003, Train: 0.7500, Val Acc: 0.4800, Test Acc: 0.4570\n",
            "Epoch: 004, Train: 0.9000, Val Acc: 0.7120, Test Acc: 0.6930\n",
            "Epoch: 005, Train: 0.9500, Val Acc: 0.7520, Test Acc: 0.7380\n",
            "Epoch: 006, Train: 0.9667, Val Acc: 0.7620, Test Acc: 0.7400\n",
            "Epoch: 007, Train: 0.9500, Val Acc: 0.7680, Test Acc: 0.7390\n",
            "Epoch: 008, Train: 0.8500, Val Acc: 0.6200, Test Acc: 0.5990\n",
            "Epoch: 009, Train: 0.9500, Val Acc: 0.7460, Test Acc: 0.7410\n",
            "Epoch: 010, Train: 0.9667, Val Acc: 0.7160, Test Acc: 0.6620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use different initial temperatures for SoftmaxAggregation"
      ],
      "metadata": {
        "id": "2lJdlP7D62wM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(12345)\n",
        "\n",
        "model = GNN(\n",
        "    hidden_channels=16,\n",
        "    aggr=MultiAggregation([\n",
        "        SoftmaxAggregation(t=0.01, learn=True),\n",
        "        SoftmaxAggregation(t=1, learn=True),\n",
        "        SoftmaxAggregation(t=100, learn=True),\n",
        "    ])\n",
        ")\n",
        "print(model)\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    loss = train(model)\n",
        "    train_acc, val_acc, test_acc = test(model)\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TTeAfY6mVNt",
        "outputId": "e438ea03-87cd-40e2-8856-683409b4f7d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (conv1): SAGEConv(500, 16, aggr=MultiAggregation([\n",
            "    SoftmaxAggregation(learn=True),\n",
            "    SoftmaxAggregation(learn=True),\n",
            "    SoftmaxAggregation(learn=True),\n",
            "  ], mode=cat))\n",
            "  (conv2): SAGEConv(16, 3, aggr=MultiAggregation([\n",
            "    SoftmaxAggregation(learn=True),\n",
            "    SoftmaxAggregation(learn=True),\n",
            "    SoftmaxAggregation(learn=True),\n",
            "  ], mode=cat))\n",
            ")\n",
            "Epoch: 001, Train: 0.8500, Val Acc: 0.7120, Test Acc: 0.7080\n",
            "Epoch: 002, Train: 0.9833, Val Acc: 0.7160, Test Acc: 0.6960\n",
            "Epoch: 003, Train: 0.7667, Val Acc: 0.6560, Test Acc: 0.6760\n",
            "Epoch: 004, Train: 0.9000, Val Acc: 0.7620, Test Acc: 0.7370\n",
            "Epoch: 005, Train: 0.9500, Val Acc: 0.7740, Test Acc: 0.7520\n",
            "Epoch: 006, Train: 0.9333, Val Acc: 0.7500, Test Acc: 0.7300\n",
            "Epoch: 007, Train: 0.9667, Val Acc: 0.8060, Test Acc: 0.7720\n",
            "Epoch: 008, Train: 0.9667, Val Acc: 0.8020, Test Acc: 0.7610\n",
            "Epoch: 009, Train: 1.0000, Val Acc: 0.7960, Test Acc: 0.7600\n",
            "Epoch: 010, Train: 0.9833, Val Acc: 0.8000, Test Acc: 0.7740\n"
          ]
        }
      ]
    }
  ]
}